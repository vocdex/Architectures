{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utilmy_deeplearning_util_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vocdex/Architectures/blob/main/utilmy_deeplearning_util_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opbHG54bPmCv",
        "outputId": "0c576016-7c84-4773-9019-dbf62c2329d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utilmy\n",
            "  Downloading utilmy-0.1.16535412-py3-none-any.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 7.1 MB/s \n",
            "\u001b[?25hCollecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting stdlib-list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from utilmy) (3.13)\n",
            "Collecting python-box\n",
            "  Downloading python_box-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire->utilmy) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->utilmy) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=775e05de1c619a502528cb0244f58669f56b69948e70ee5f6368a9e06d75c122\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: stdlib-list, python-box, fire, utilmy\n",
            "Successfully installed fire-0.4.0 python-box-6.0.2 stdlib-list-0.8.0 utilmy-0.1.16535412\n"
          ]
        }
      ],
      "source": [
        "!pip install utilmy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import utilmy"
      ],
      "metadata": {
        "id": "2bxrmxsmXb-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.debugger import set_trace"
      ],
      "metadata": {
        "id": "JcFRm3yhVpRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Original code"
      ],
      "metadata": {
        "id": "_s8m2piFVQaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7Hrkpe1FD2hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"# \n",
        "Doc::\n",
        "\n",
        "    Embedding utils/ Visualization.\n",
        "      https://try2explore.com/questions/10109123\n",
        "      https://mpld3.github.io/examples/index.html\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "import os, glob, sys, math, time, json, functools, random, yaml, gc, copy, pandas as pd, numpy as np\n",
        "import datetime\n",
        "from pathlib import Path; from collections import defaultdict, OrderedDict ;\n",
        "from typing import List, Optional, Tuple, Union  ; from numpy import ndarray\n",
        "from box import Box\n",
        "\n",
        "import warnings ;warnings.filterwarnings(\"ignore\")\n",
        "from warnings import simplefilter  ; simplefilter(action='ignore', category=FutureWarning)\n",
        "with warnings.catch_warnings():\n",
        "    import matplotlib.pyplot as plt\n",
        "    import mpld3\n",
        "    from scipy.cluster.hierarchy import ward, dendrogram\n",
        "    import sklearn\n",
        "\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.manifold import MDS\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "from utilmy import pd_read_file, os_makedirs, pd_to_file, glob_glob\n",
        "\n",
        "\n",
        "#### Optional imports\n",
        "try :\n",
        "    import hdbscan, umap\n",
        "    import faiss\n",
        "    import diskcache as dc\n",
        "\n",
        "except: pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#############################################################################################\n",
        "from utilmy import log, log2, os_module_name\n",
        "MNAME = os_module_name(__file__)\n",
        "\n",
        "def help():\n",
        "    \"\"\"function help        \"\"\"\n",
        "    from utilmy import help_create\n",
        "    print( help_create(__file__) )\n",
        "\n",
        "\n",
        "\n",
        "#############################################################################################\n",
        "def test_all() -> None:\n",
        "    \"\"\" python  $utilmy/deeplearning/util_embedding.py test_all         \"\"\"\n",
        "    log(MNAME)\n",
        "    test1()\n",
        "\n",
        "\n",
        "def test1() -> None:\n",
        "    \"\"\"function test1     \n",
        "    \"\"\"\n",
        "    d = Box({})\n",
        "    dirtmp =\"./ztmp/\"\n",
        "    embedding_create_vizhtml(dirin=dirtmp + \"/model.vec\", dirout=dirtmp + \"out/\", dim_reduction='umap', nmax=100, ntrain=10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_create_fake_df():\n",
        "    \"\"\" Creates a fake embeddingdataframe\n",
        "    \"\"\"\n",
        "    n = 30\n",
        "\n",
        "    emb_list = []\n",
        "    # Create fake user ids\n",
        "    userid = [i for i in range(n)]\n",
        "\n",
        "    for i in range(n):\n",
        "        emb_list.append( ','.join([str(x) for x in np.random.random( (0,1,120)) ])  )\n",
        "\n",
        "    # Populate a dataframe with fake data\n",
        "    df = pd.DataFrame()\n",
        "    df['wordid']  = userid\n",
        "    df['emb']     = emb_list\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################################################################################\n",
        "############### Visualize the embeddings ################################################################\n",
        "def embedding_create_vizhtml(dirin=\"in/model.vec\", dirout=\"ztmp/\", dim_reduction='umap', nmax=100, ntrain=10):\n",
        "   \"\"\"Create HTML plot file of embeddings.\n",
        "   Doc::\n",
        "\n",
        "        dirin= \"  .parquet OR  Word2vec .vec  OR  .pkl  file\"\n",
        "        embedding_create_vizhtml(dirin=\"in/model.vec\", dirout=\"zhtmlfile/\", dim_reduction='umap', nmax=100, ntrain=10)\n",
        "\n",
        "\n",
        "   \"\"\"\n",
        "   tag     = f\"{nmax}_{dim_reduction}\"\n",
        "\n",
        "   #### Generate HTML  ############################################\n",
        "   log(dirin)\n",
        "\n",
        "   myviz = EmbeddingViz(path = dirin)\n",
        "   myviz.load_data(nmax= nmax)\n",
        "   myviz.run_all(dirout= dirout, dim_reduction=dim_reduction, nmax=nmax, ntrain=ntrain)\n",
        "\n",
        "\n",
        "\n",
        "class EmbeddingViz:\n",
        "    def __init__(self, path=\"myembed.parquet\", num_clusters=5, sep=\";\", config:dict=None):\n",
        "        \"\"\" Visualize Embedding\n",
        "        Doc::\n",
        "\n",
        "                Many issues with numba, numpy, pyarrow !!!!\n",
        "                pip install  pynndescent==0.5.4  numba==0.53.1  umap-learn==0.5.1  llvmlite==0.36.0   numpy==1.19.1   --no-deps\n",
        "\n",
        "                myviz = vizEmbedding(path = \"C:/D/gitdev/cpa/data/model.vec\")\n",
        "                myviz.run_all(nmax=5000)\n",
        "\n",
        "                myviz.dim_reduction(mode='mds')\n",
        "                myviz.create_visualization(dir_out=\"ztmp/vis/\")\n",
        "\n",
        "        \"\"\"\n",
        "        self.path         = path\n",
        "        self.sep          = sep\n",
        "        self.num_clusters = num_clusters\n",
        "        self.dist         = None\n",
        "\n",
        "        ### Plot @D coordinate\n",
        "        self.coordinate_xy = None\n",
        "\n",
        "        ### Store the embeddings\n",
        "        self.id_map    = None\n",
        "        self.df_labels = None\n",
        "        self.embs      = None\n",
        "\n",
        "\n",
        "    def run_all(self, dim_reduction=\"mds\", col_embed='embed', ndim=2, nmax= 5000, dirout=\"ztmp/\", ntrain=10000):\n",
        "       self.dim_reduction(dim_reduction, ndim=ndim, nmax= nmax, dir_out=dirout, ntrain=ntrain)\n",
        "       self.create_clusters(after_dim_reduction=True)\n",
        "       self.create_visualization(dirout, mode='d3', cols_label=None, show_server=False)\n",
        "\n",
        "\n",
        "    def load_data(self,  col_embed='embed', nmax= 5000,  npool=2 ):\n",
        "        \"\"\"  Load embedding vector from file.\n",
        "        Doc::\n",
        "\n",
        "                ip_map :     dict  0--N  Integer to  id_label\n",
        "                df_labelss : pandas dataframe: id, label1, label2\n",
        "                embs  :      list of np array\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        if \".vec\"     in self.path :\n",
        "          embs, id_map, df_labels  = embedding_load_word2vec(self.path, nmax= nmax)\n",
        "\n",
        "        if \".pkl\" in self.path :\n",
        "          embs, id_map, df_labels  = embedding_load_pickle(self.path, nmax= nmax)\n",
        "\n",
        "        else : # if \".parquet\" in self.path :\n",
        "          embs, id_map, df_labels  = embedding_load_parquet(self.path, nmax= nmax)\n",
        "\n",
        "        assert isinstance(id_map, dict)\n",
        "        assert isinstance(df_labels, pd.DataFrame)\n",
        "        assert isinstance(embs, np.ndarray) or isinstance(embs, list)\n",
        "\n",
        "        self.id_map    = id_map\n",
        "        self.df_labels = df_labels\n",
        "        self.embs      = embs\n",
        "\n",
        "\n",
        "    def dim_reduction(self, mode=\"mds\", ndim=2, nmax= 5000, dirout=None, ntrain=10000, npool=2):\n",
        "        \"\"\"  Reduce dimension of embedding into 2D X,Y for plotting.\n",
        "        Doc::\n",
        "\n",
        "             mode:   'mds', 'umap'  algo reduction.\n",
        "             ntrain: 10000, nb of samples to train.\n",
        "            \n",
        "        \"\"\"\n",
        "        pos = None\n",
        "        if mode == 'mds' :\n",
        "            ### Co-variance matrix\n",
        "            dist = 1 - cosine_similarity(self.embs)\n",
        "            mds = MDS(n_components=ndim, dissimilarity=\"precomputed\", random_state=1)\n",
        "            mds.fit(dist)  # shape (n_components, n_samples)\n",
        "            pos = mds.transform(dist)  # shape (n_components, n_samples)\n",
        "\n",
        "\n",
        "        if mode == 'umap' :\n",
        "            y_label = None\n",
        "            from umap import UMAP, AlignedUMAP, ParametricUMAP\n",
        "            clf = UMAP( set_op_mix_ratio=0.25, ## Preserve outlier\n",
        "                        densmap=False, dens_lambda=5.0,          ## Preserve density\n",
        "                        n_components= ndim,\n",
        "                        n_neighbors=7,  metric='euclidean',\n",
        "                        metric_kwds=None, output_metric='euclidean',\n",
        "                        output_metric_kwds=None, n_epochs=None,\n",
        "                        learning_rate=1.0, init='spectral',\n",
        "                        min_dist=0.0, spread=1.0, low_memory=True, n_jobs= npool,\n",
        "                        local_connectivity=1.0,\n",
        "                        repulsion_strength=1.0, negative_sample_rate=5,\n",
        "                        transform_queue_size=4.0, a=None, b=None, random_state=None,\n",
        "                        angular_rp_forest=False, target_n_neighbors=-1,\n",
        "                        target_metric='categorical', target_metric_kwds=None,\n",
        "                        target_weight=0.5, transform_seed=42, transform_mode='embedding',\n",
        "                        force_approximation_algorithm= True, verbose=False,\n",
        "                        unique=False,  dens_frac=0.3,\n",
        "                        dens_var_shift=0.1, output_dens=False, disconnection_distance=None)\n",
        "\n",
        "            clf.fit(self.embs[ np.random.choice(len(self.embs), size= ntrain)  , :], y=y_label)\n",
        "            pos  = clf.transform( self.embs )\n",
        "\n",
        "        self.coordinate_xy       = pos\n",
        "\n",
        "        if dirout is not None :\n",
        "            os.makedirs(dirout, exist_ok=True)\n",
        "            df = pd.DataFrame(pos, columns=['x', 'y'] )\n",
        "            for ci in [ 'x', 'y' ] :\n",
        "               df[ ci ] = df[ ci ].astype('float32')\n",
        "\n",
        "            # log(df, df.dtypes)\n",
        "            pd_to_file(df.iloc[:100, :],  f\"{dirout}/embs_xy_{mode}.csv\" )\n",
        "            pd_to_file(df,                f\"{dirout}/embs_xy_{mode}.parquet\" , show=1)\n",
        "\n",
        "\n",
        "    def create_clusters(self, method='kmeans', after_dim_reduction=True):\n",
        "\n",
        "        import hdbscan\n",
        "        #km = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=10)  #.fit_predict(self.pos)\n",
        "        km = KMeans(n_clusters=self.num_clusters)\n",
        "\n",
        "        if after_dim_reduction :\n",
        "           km.fit(self.coordinate_xy)\n",
        "        else :\n",
        "           km.fit( self.embs)\n",
        "\n",
        "\n",
        "        self.clusters      = km.labels_.tolist()\n",
        "        self.cluster_color = [f'#{random.randint(0, 0xFFFFFF):06x}' for _ in range(self.num_clusters)]\n",
        "        self.cluster_names = {i: f'Cluster {i}' for i in range(self.num_clusters)}\n",
        "\n",
        "\n",
        "    def create_visualization(self, dir_out=\"ztmp/\", mode='d3', cols_label=None, start_server=False,  **kw ):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        os.makedirs(dir_out, exist_ok=True)\n",
        "        cols_label          = [] if cols_label is None else cols_label\n",
        "        text_label_and_text = []\n",
        "        for i,x in self.df_labels.iterrows():\n",
        "          ss = x[\"id\"]\n",
        "          for ci in cols_label:\n",
        "             ss = ss + \":\" + x[ci]\n",
        "          text_label_and_text.append(ss)\n",
        "\n",
        "        #######################################################################################\n",
        "        # create data frame that has the result of the MDS plus the cluster numbers and titles\n",
        "        df = pd.DataFrame(dict(x=self.coordinate_xy[:, 0],\n",
        "                               y=self.coordinate_xy[:, 1],\n",
        "                               clusters= self.clusters, title=text_label_and_text))\n",
        "        df.to_parquet(f\"{dir_out}/embs_xy_cluster.parquet\")\n",
        "\n",
        "\n",
        "        # group by cluster\n",
        "        groups_clusters = df.groupby('clusters')\n",
        "\n",
        "        # set up plot\n",
        "        fig, ax = plt.subplots(figsize=(25, 15))  # set size\n",
        "        ax.margins(0.05)  # Optional, just adds 5% padding to the autoscaling\n",
        "\n",
        "        # iterate through groups to layer the plot\n",
        "        # note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return\n",
        "        # the appropriate color/label\n",
        "        for name, group in groups_clusters:\n",
        "            ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label= self.cluster_names[name],\n",
        "                    color=self.cluster_color[name],\n",
        "                    mec='none')\n",
        "            ax.set_aspect('auto')\n",
        "            ax.tick_params(axis='x',  # changes apply to the x-axis\n",
        "                           which='both',  # both major and minor ticks are affected\n",
        "                           bottom='off',  # ticks along the bottom edge are off\n",
        "                           top='off',  # ticks along the top edge are off\n",
        "                           labelbottom='off')\n",
        "            ax.tick_params(axis='y',  # changes apply to the y-axis\n",
        "                           which='both',  # both major and minor ticks are affected\n",
        "                           left='off',  # ticks along the bottom edge are off\n",
        "                           top='off',  # ticks along the top edge are off\n",
        "                           labelleft='off')\n",
        "\n",
        "        ax.legend(numpoints=1)  # show legend with only 1 point\n",
        "\n",
        "        # add label in x,y position with the label as the\n",
        "        for i in range(len(df)):\n",
        "            ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], size=8)\n",
        "\n",
        "        # uncomment the below to save the plot if need be\n",
        "        plt.savefig(f'{dir_out}/clusters_static-{datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")}.png', dpi=200)\n",
        "\n",
        "        # Plot\n",
        "        fig, ax = plt.subplots(figsize=(20, 15))  # set plot size\n",
        "        ax.margins(0.03)  # Optional, just adds 5% padding to the autoscaling\n",
        "\n",
        "        # iterate through groups to layer the plot\n",
        "        for name, group in groups_clusters:\n",
        "            points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=7, label= self.cluster_names[name], mec='none',\n",
        "                             color=self.cluster_color[name])\n",
        "            ax.set_aspect('auto')\n",
        "            labels = [i for i in group.title]\n",
        "\n",
        "            # set tooltip using points, labels and the already defined 'css'\n",
        "            tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels, voffset=10, hoffset=10, css=CSS)\n",
        "            # connect tooltip to fig\n",
        "            mpld3.plugins.connect(fig, tooltip, TopToolbar())\n",
        "\n",
        "            # set tick marks as blank\n",
        "            ax.axes.get_xaxis().set_ticks([])\n",
        "            ax.axes.get_yaxis().set_ticks([])\n",
        "\n",
        "            # set axis as blank\n",
        "            ax.axes.get_xaxis().set_visible(False)\n",
        "            ax.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "        ax.legend(numpoints=1)  # show legend with only one dot\n",
        "\n",
        "\n",
        "        ##### Export ############################################################\n",
        "        mpld3.save_html(fig,  f\"{dir_out}/embeds.html\")\n",
        "        log(f\"{dir_out}/embeds.html\" )\n",
        "\n",
        "        ### Windows specifc\n",
        "        if os.name == 'nt': os.system(f'start chrome \"{dir_out}/embeds.html\" ')\n",
        "\n",
        "\n",
        "        if start_server :\n",
        "           # mpld3.show(fig=None, ip='127.0.0.1', port=8888, n_retries=50, local=True, open_browser=True, http_server=None, **kwargs)[source]\n",
        "           mpld3.show()  # show the plot\n",
        "\n",
        "\n",
        "    def draw_cluster_hiearchy(self):\n",
        "        \"\"\"  Dendogram from distance\n",
        "\n",
        "        \"\"\"\n",
        "        from scipy.cluster.hierarchy import ward, dendrogram\n",
        "        linkage_matrix = ward(self.dist)  # define the linkage_matrix using ward clustering pre-computed distances\n",
        "        fig, ax = plt.subplots(figsize=(15, 20))  # set size\n",
        "        ax = dendrogram(linkage_matrix, orientation=\"right\", labels=self.text_labels)\n",
        "        plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('dendogram_clusters.png', dpi=200)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################################################################################\n",
        "############## Loader of embeddings #####################################################################\n",
        "def embedding_torchtensor_to_parquet(tensor_list,\n",
        "                                     id_list:list, label_list, dirout=None, tag=\"\",  nmax=10 ** 8 ):\n",
        "    \"\"\" List ofTorch tensor to embedding stored in parquet\n",
        "    Doc::\n",
        "\n",
        "        yemb = model.encode(X)\n",
        "        id_list = np.arange(0, len(yemb))\n",
        "        ylabel = ytrue\n",
        "        embedding_torchtensor_to_parquet(tensor_list= yemb,\n",
        "                                     id_list=id_list, label_list=ylabel,\n",
        "                                     dirout=\"./ztmp/\", tag=\"v01\"  )\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    n          =  len(tensor_list)\n",
        "    id_list    = np.arange(0, n) if id_list is None else id_list\n",
        "    label_list = [0]*n if label_list is None else id_list\n",
        "\n",
        "    assert len(id_list) == len(tensor_list)\n",
        "\n",
        "    df = []\n",
        "    for idi, vecti, labeli in zip(id_list,tensor_list, label_list):\n",
        "        ss = np_array_to_str(vecti.tonumpy())\n",
        "        df.append([ idi, ss, labeli    ])\n",
        "\n",
        "    df = pd.DataFrame(df, columns= ['id', 'emb', 'label'])\n",
        "\n",
        "\n",
        "    if dirout is not None :\n",
        "      log(dirout) ; os_makedirs(dirout)  ; time.sleep(4)\n",
        "      dirout2 = dirout + f\"/df_emb_{tag}.parquet\"\n",
        "      pd_to_file(df, dirout2, show=1 )\n",
        "    return df\n",
        "\n",
        "\n",
        "def embedding_rawtext_to_parquet(dirin=None, dirout=None, skip=0, nmax=10 ** 8,\n",
        "                                 is_linevalid_fun=None):   ##   python emb.py   embedding_to_parquet  &\n",
        "    #### FastText/ Word2Vec to parquet files    9808032 for purhase\n",
        "    log(dirout) ; os_makedirs(dirout)  ; time.sleep(4)\n",
        "\n",
        "    if is_linevalid_fun is None : #### Validate line\n",
        "        def is_linevalid_fun(w):\n",
        "            return len(w)> 5  ### not too small tag\n",
        "\n",
        "    i = 0; kk=-1; words =[]; embs= []; ntot=0\n",
        "    with open(dirin, mode='r') as fp:\n",
        "        while i < nmax+1  :\n",
        "            i  = i + 1\n",
        "            ss = fp.readline()\n",
        "            if not ss  : break\n",
        "            if i < skip: continue\n",
        "\n",
        "            ss = ss.strip().split(\" \")            \n",
        "            if not is_linevalid_fun(ss[0]): continue\n",
        "\n",
        "            words.append(ss[0])\n",
        "            embs.append( \",\".join(ss[1:]) )\n",
        "\n",
        "            if i % 200000 == 0 :\n",
        "              kk = kk + 1                \n",
        "              df = pd.DataFrame({ 'id' : words, 'emb' : embs }  )  \n",
        "              log(df.shape, ntot)  \n",
        "              if i < 2: log(df)  \n",
        "              pd_to_file(df, dirout + f\"/df_emb_{kk}.parquet\", show=0)\n",
        "              ntot += len(df)\n",
        "              words, embs = [], []  \n",
        "\n",
        "    kk      = kk + 1                \n",
        "    df      = pd.DataFrame({ 'id' : words, 'emb' : embs }  )  \n",
        "    ntot   += len(df)\n",
        "    dirout2 = dirout + f\"/df_emb_{kk}.parquet\"\n",
        "    pd_to_file(df, dirout2, show=1 )\n",
        "    log('ntotal', ntot, dirout2 )\n",
        "    return os.path.dirname(dirout2)\n",
        "\n",
        "\n",
        "\n",
        "def embedding_load_parquet(dirin=\"df.parquet\",  colid= 'id', col_embed= 'emb',  nmax= 500):\n",
        "    \"\"\"  Required columns : id, emb (string , separated)\n",
        "    \n",
        "    \"\"\"\n",
        "    log('loading', dirin)\n",
        "    flist = list( glob.glob(dirin) )\n",
        "    \n",
        "    df  = pd_read_file( flist, npool= max(1, int( len(flist) / 4) ) )\n",
        "    nmax    = nmax if nmax > 0 else  len(df)   ### 5000\n",
        "    df  = df.iloc[:nmax, :]\n",
        "    df  = df.rename(columns={ col_embed: 'emb'})\n",
        "    \n",
        "    df  = df[ df['emb'].apply( lambda x: len(x)> 10  ) ]  ### Filter small vector\n",
        "    log(df.head(5).T, df.columns, df.shape)\n",
        "    log(df, df.dtypes)    \n",
        "\n",
        "\n",
        "    ###########################################################################\n",
        "    ###### Split embed numpy array, id_map list,  #############################\n",
        "    embs    = np_str_to_array(df['emb'].values,  l2_norm=True,     mdim = 200)\n",
        "    id_map  = { name: i for i,name in enumerate(df[colid].values) }     \n",
        "    log(\",\", str(embs)[:50], \",\", str(id_map)[:50] )\n",
        "    \n",
        "    #####  Keep only label infos  ####\n",
        "    del df['emb']                  \n",
        "    return embs, id_map, df \n",
        "\n",
        "\n",
        "\n",
        "def embedding_load_word2vec(dirin=None, skip=0, nmax=10 ** 8,\n",
        "                                 is_linevalid_fun=None):\n",
        "    \"\"\"  Parse FastText/ Word2Vec to parquet files.\n",
        "    Doc::\n",
        "\n",
        "       dirin: .parquet files with cols:\n",
        "       embs: 2D np.array, id_map: Dict, dflabel: pd.DataFrame\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    if is_linevalid_fun is None : #### Validate line\n",
        "        def is_linevalid_fun(w):\n",
        "            return len(w)> 5  ### not too small tag\n",
        "\n",
        "    i = 0; kk=-1; words =[]; embs= []; ntot=0\n",
        "    with open(dirin, mode='r') as fp:\n",
        "        while i < nmax+1  :\n",
        "            i  = i + 1\n",
        "            ss = fp.readline()\n",
        "            if not ss  : break\n",
        "            if i < skip: continue\n",
        "\n",
        "            ss = ss.strip().split(\" \")\n",
        "            if not is_linevalid_fun(ss[0]): continue\n",
        "\n",
        "            words.append(ss[0])\n",
        "            embs.append( \",\".join(ss[1:]) )\n",
        "\n",
        "\n",
        "    kk      = kk + 1\n",
        "    df      = pd.DataFrame({ 'id' : words, 'emb' : embs }  )\n",
        "    ntot   += len(df)\n",
        "\n",
        "\n",
        "    embs   =  np_str_to_array( df['emb'].values  )  ### 2D numpy array\n",
        "    id_map = { i : idi for i, idi in enumerate(df['id'].values)  }\n",
        "    dflabel      = pd.DataFrame({ 'id' : words }  )\n",
        "    dflabel['label1'] = 0\n",
        "\n",
        "    return  embs, id_map, dflabel\n",
        "\n",
        "\n",
        "\n",
        "def embedding_load_pickle(dirin=None, skip=0, nmax=10 ** 8,\n",
        "                                 is_linevalid_fun=None):   ##   python emb.py   embedding_to_parquet  &\n",
        "    \"\"\"\n",
        "       Load pickle from disk into embs, id_map, dflabel\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "\n",
        "    embs = None\n",
        "    flist =  glob_glob(dirin)\n",
        "    for fi in flist :\n",
        "        arr = pickle.load(fi)\n",
        "        embs = np.concatenate((embs, arr)) if embs is not None else arr\n",
        "\n",
        "\n",
        "    id_map  = {i: i for i in  range(0, len(embs))}\n",
        "    dflabel = pd.DataFrame({'id': [] })\n",
        "    return embs, id_map, dflabel\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def embedding_extract_fromtransformer(model,Xinput:list):\n",
        "    \"\"\" Transformder require Pooling layer to extract word level embedding.\n",
        "    Doc::\n",
        "\n",
        "        https://github.com/Riccorl/transformers-embedder\n",
        "        import transformers_embedder as tre\n",
        "\n",
        "        tokenizer = tre.Tokenizer(\"bert-base-cased\")\n",
        "\n",
        "        model = tre.TransformersEmbedder(\n",
        "            \"bert-base-cased\", subword_pooling_strategy=\"sparse\", layer_pooling_strategy=\"mean\"\n",
        "        )\n",
        "\n",
        "        example = \"This is a sample sentence\"\n",
        "        inputs = tokenizer(example, return_tensors=True)\n",
        "\n",
        "\n",
        "        class TransformersEmbedder(torch.nn.Module):\n",
        "                model: Union[str, tr.PreTrainedModel],\n",
        "                subword_pooling_strategy: str = \"sparse\",\n",
        "                layer_pooling_strategy: str = \"last\",\n",
        "                output_layers: Tuple[int] = (-4, -3, -2, -1),\n",
        "                fine_tune: bool = True,\n",
        "                return_all: bool = True,\n",
        "            )\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    import transformers_embedder as tre\n",
        "\n",
        "    tokenizer = tre.Tokenizer(\"bert-base-cased\")\n",
        "\n",
        "    model = tre.TransformersEmbedder(\n",
        "        \"bert-base-cased\", subword_pooling_strategy=\"sparse\", layer_pooling_strategy=\"mean\"\n",
        "    )\n",
        "\n",
        "    # X = \"This is a sample sentence\"\n",
        "    X2 = tokenizer(Xinput, return_tensors=True)\n",
        "    yout = model(X2)\n",
        "    emb  = yout.word_embeddings.shape[1:-1]       # remove [CLS] and [SEP]\n",
        "    # torch.Size([1, 5, 768])\n",
        "    # len(example)\n",
        "    return yout\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################################################################\n",
        "######## Top-K retrieval ###############################################################################\n",
        "def sim_scores_pairwise(embs:np.ndarray, word_list:list, is_symmetric=False):\n",
        "    \"\"\" Pairwise Cosinus Sim scores\n",
        "    Example:\n",
        "        Doc::\n",
        "\n",
        "           embs   = np.random.random((10,200))\n",
        "           idlist = [str(i) for i in range(0,10)]\n",
        "           df = sim_scores_fast(embs:np, idlist, is_symmetric=False)\n",
        "           df[[ 'id1', 'id2', 'sim_score'  ]]\n",
        "\n",
        "    \"\"\"\n",
        "    from sklearn.metrics.pairwise import cosine_similarity    \n",
        "    dfsim = []\n",
        "    for i in  range(0, len(word_list) - 1) :\n",
        "        vi = embs[i,:]\n",
        "        normi = np.sqrt(np.dot(vi,vi))\n",
        "        for j in range(i+1, len(word_list)) :\n",
        "            # simij = cosine_similarity( embs[i,:].reshape(1, -1) , embs[j,:].reshape(1, -1)     )\n",
        "            vj = embs[j,:]\n",
        "            normj = np.sqrt(np.dot(vj, vj))\n",
        "            simij = np.dot( vi ,  vj  ) / (normi * normj)\n",
        "            dfsim.append([ word_list[i], word_list[j],  simij   ])\n",
        "            # dfsim2.append([ nwords[i], nwords[j],  simij[0][0]  ])\n",
        "    \n",
        "    dfsim  = pd.DataFrame(dfsim, columns= ['id1', 'id2', 'sim_score' ] )   \n",
        "\n",
        "    if is_symmetric:\n",
        "        ### Add symmetric part      \n",
        "        dfsim3 = copy.deepcopy(dfsim)\n",
        "        dfsim3.columns = ['id2', 'id1', 'sim_score' ] \n",
        "        dfsim          = pd.concat(( dfsim, dfsim3 ))\n",
        "    return dfsim\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "def topk_nearest_vector(x0:np.ndarray, vector_list:list, topk=3, engine='faiss', engine_pars:dict=None) :\n",
        "    \"\"\" Retrieve top k nearest vectors using FAISS, raw retrievail\n",
        "    \"\"\"\n",
        "    if 'faiss' in engine :\n",
        "        # cc = engine_pars\n",
        "        import faiss  \n",
        "        index = faiss.index_factory(x0.shape[1], 'Flat')\n",
        "        index.add(vector_list)\n",
        "        dist, indice = index.search(x0, topk)\n",
        "        return dist, indice\n",
        "\n",
        "\n",
        "\n",
        "def topk_calc( diremb=\"\", dirout=\"\", topk=100,  idlist=None, nexample=10, emb_dim=200, tag=None, debug=True):\n",
        "    \"\"\" Get Topk vector per each element vector of dirin.\n",
        "    Example:\n",
        "        Doc::\n",
        "\n",
        "           Return  pd.DataFrame( columns=[  'id', 'emb', 'topk', 'dist'  ] )\n",
        "             id : id of the emb\n",
        "             emb : [342,325345,343]   X0 embdding\n",
        "             topk:  2,5,6,5,6\n",
        "             distL 0,3423.32424.,\n",
        "\n",
        "    \n",
        "           python $utilmy/deeplearning/util_embedding.py  topk_calc   --diremb     --dirout\n",
        "    \n",
        "\n",
        "    \"\"\"\n",
        "    from utilmy import pd_read_file\n",
        "\n",
        "    ##### Load emb data  ###############################################\n",
        "    flist    = glob_glob(diremb)\n",
        "    df       = pd_read_file(  flist , n_pool=10 )\n",
        "    df.index = np.arange(0, len(df))\n",
        "    log(df)\n",
        "\n",
        "    assert len(df[['id', 'emb' ]]) > 0\n",
        "\n",
        "\n",
        "    ##### Element X0 ####################################################\n",
        "    vectors = np_str_to_array(df['emb'].values,  mdim= emb_dim)\n",
        "    del df ; gc.collect()\n",
        "\n",
        "    llids = idlist\n",
        "    if idlist is None :    \n",
        "       llids = df['id'].values    \n",
        "       llids = llids[:nexample]\n",
        "\n",
        "    dfr = [] \n",
        "    for ii in range(0, len(llids)) :        \n",
        "        x0      = vectors[ii]\n",
        "        xname   = llids[ii]\n",
        "        log(xname)\n",
        "        x0         = x0.reshape(1, -1).astype('float32')  \n",
        "        dist, rank = topk_nearest_vector(x0, vectors, topk= topk) \n",
        "        \n",
        "        ss_rankid = np_array_to_str( llids[ rank[0] ] )\n",
        "        ss_distid = np_array_to_str( dist[0]  )\n",
        "\n",
        "        dfr.append([  xname, x0,  ss_rankid,  ss_distid  ])   \n",
        "\n",
        "    dfr = pd.DataFrame( dfr, columns=[  'id', 'emb', 'topk', 'dist'  ] )\n",
        "    pd_read_file( dfr, dirout + f\"/topk_{tag}.parquet\"  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################################################################\n",
        "######## Top-K retrieval Faiss #########################################################################\n",
        "def faiss_create_index(df_or_path=None, col='emb', dirout=None,  db_type = \"IVF4096,Flat\", nfile=1000, emb_dim=200,\n",
        "                       nrows=-1):\n",
        "    \"\"\" 1 billion size vector Index creation\n",
        "    Docs::\n",
        "\n",
        "          python util_embedding.py   faiss_create_index    --df_or_path myemb/\n",
        "    \"\"\"\n",
        "    import faiss\n",
        "\n",
        "    \n",
        "    dirout    =  \"/\".join( os.path.dirname(df_or_path).split(\"/\")[:-1]) + \"/faiss/\" if dirout is None else dirout\n",
        "\n",
        "    os.makedirs(dirout, exist_ok=True) ; \n",
        "    log( 'dirout', dirout)    \n",
        "    log('dirin',   df_or_path)  ; time.sleep(10)\n",
        "    \n",
        "    if isinstance(df_or_path, str) :      \n",
        "       flist = sorted(glob.glob(df_or_path  ))[:nfile] \n",
        "       log('Loading', df_or_path) \n",
        "       df = pd_read_file(flist, n_pool=20, verbose=False)\n",
        "    else :\n",
        "       df = df_or_path\n",
        "\n",
        "    df  = df.iloc[:nrows, :]   if nrows>0  else df\n",
        "    log(df)\n",
        "        \n",
        "    tag = f\"_\" + str(len(df))    \n",
        "    df  = df.sort_values('id')    \n",
        "    df[ 'idx' ] = np.arange(0,len(df))\n",
        "    pd_to_file( df[[ 'idx', 'id' ]], \n",
        "                dirout + f\"/map_idx{tag}.parquet\", show=1)   #### Keeping maping faiss idx, item_tag\n",
        "    \n",
        "\n",
        "    log(\"#### Convert parquet to numpy   \", dirout)\n",
        "    X  = np.zeros((len(df), emb_dim  ), dtype=np.float32 )    \n",
        "    vv = df[col].values\n",
        "    del df; gc.collect()\n",
        "    for i, r in enumerate(vv) :\n",
        "        try :\n",
        "          vi      = [ float(v) for v in r.split(',')]        \n",
        "          X[i, :] = vi\n",
        "        except Exception as e:\n",
        "          log(i, e)\n",
        "            \n",
        "    log(\"#### Preprocess X\")\n",
        "    faiss.normalize_L2(X)  ### Inplace L2 normalization\n",
        "    log( X ) \n",
        "    \n",
        "    nt = min(len(X), int(max(400000, len(X) *0.075 )) )\n",
        "    Xt = X[ np.random.randint(len(X), size=nt),:]\n",
        "    log('Nsample training', nt)\n",
        "\n",
        "    ####################################################    \n",
        "    D = emb_dim   ### actual  embedding size\n",
        "    N = len(X)   #1000000\n",
        "\n",
        "    # Param of PQ for 1 billion\n",
        "    M      = 40 # 16  ###  200 / 5 = 40  The number of sub-vector. Typically this is 8, 16, 32, etc.\n",
        "    nbits  = 8        ### bits per sub-vector. This is typically 8, so that each sub-vec is encoded by 1 byte    \n",
        "    nlist  = 6000     ###  # Param of IVF,  Number of cells (space partition). Typical value is sqrt(N)    \n",
        "    hnsw_m = 32       ###  # Param of HNSW Number of neighbors for HNSW. This is typically 32\n",
        "\n",
        "    # Setup  distance -> similarity in uncompressed space is  dis = 2 - 2 * sim, https://github.com/facebookresearch/faiss/issues/632\n",
        "    quantizer = faiss.IndexHNSWFlat(D, hnsw_m)\n",
        "    index     = faiss.IndexIVFPQ(quantizer, D, nlist, M, nbits)\n",
        "    \n",
        "    log('###### Train indexer')\n",
        "    index.train(Xt)      # Train\n",
        "    \n",
        "    log('###### Add vectors')\n",
        "    index.add(X)        # Add\n",
        "\n",
        "    log('###### Test values ')\n",
        "    index.nprobe = 8  # Runtime param. The number of cells that are visited for search.\n",
        "    dists, ids = index.search(x=X[:3], k=4 )  ## top4\n",
        "    log(dists, ids)\n",
        "    \n",
        "    log(\"##### Save Index    \")\n",
        "    dirout2 = dirout + f\"/faiss_trained{tag}.index\" \n",
        "    log( dirout2 )\n",
        "    faiss.write_index(index, dirout2 )\n",
        "    return dirout2\n",
        "        \n",
        "\n",
        "\n",
        "def faiss_load_index(faiss_index_path=\"\"):\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "def faiss_topk_calc(df=None, root=None, colid='id', colemb='emb', faiss_index=None, topk=200, npool=1, nrows=10**7, nfile=1000) :  ##  python prepro.py  faiss_topk   2>&1 | tee -a zlog_faiss_topk.txt\n",
        "   \"\"\"#\n",
        "   Doc::\n",
        "   \n",
        "       id, dist_list, id_list \n",
        "       \n",
        "       https://github.com/facebookresearch/faiss/issues/632\n",
        "       \n",
        "       This represents the quantization error for vectors inside the dataset.\n",
        "        For vectors in denser areas of the space, the quantization error is lower because the quantization centroids are bigger and vice versa.\n",
        "        Therefore, there is no limit to this error that is valid over the whole space. However, it is possible to recompute the exact distances once you have the nearest neighbors, by accessing the uncompressed vectors.\n",
        "\n",
        "        distance -> similarity in uncompressed space is\n",
        "\n",
        "        dis = 2 - 2 * sim\n",
        "  \n",
        "   \"\"\"\n",
        "   # nfile  = 1000      ; nrows= 10**7\n",
        "   # topk   = 500 \n",
        " \n",
        "   if faiss_index is None : \n",
        "      faiss_index = \"\"  \n",
        "      # faiss_index = root + \"/faiss/faiss_trained_9808032.index\"\n",
        "   log('Faiss Index: ', faiss_index)\n",
        "   if isinstance(faiss_index, str) :\n",
        "        faiss_path  = faiss_index\n",
        "        faiss_index = faiss_load_index(db_path=faiss_index) \n",
        "   faiss_index.nprobe = 12  # Runtime param. The number of cells that are visited for search.\n",
        "        \n",
        "   ########################################################################\n",
        "   if isinstance(df, list):    ### Multi processing part\n",
        "        if len(df) < 1 : return 1\n",
        "        flist = df[0]\n",
        "        root     = os.path.abspath( os.path.dirname( flist[0] + \"/../../\") )  ### bug in multipro\n",
        "        dirin    = root + \"/df/\"\n",
        "        dir_out  = root + \"/topk/\"\n",
        "\n",
        "   elif df is None : ## Default\n",
        "        root    = \"emb/emb/i_1000000000/\"\n",
        "        dirin   = root + \"/df/*.parquet\"        \n",
        "        dir_out = root + \"/topk/\"\n",
        "        flist = sorted(glob.glob(dirin))\n",
        "                \n",
        "   else : ### df == string path\n",
        "        root    = os.path.abspath( os.path.dirname(df)  + \"/../\") \n",
        "        log(root)\n",
        "        dirin   = root + \"/df/*.parquet\"\n",
        "        dir_out = root + \"/topk/\"  \n",
        "        flist   = sorted(glob.glob(dirin))\n",
        "        \n",
        "   log('dir_in',  dirin) ;        \n",
        "   log('dir_out', dir_out) ; time.sleep(2)     \n",
        "   flist = flist[:nfile]\n",
        "   if len(flist) < 1: return 1 \n",
        "   log('Nfile', len(flist), flist )\n",
        "   # return 1\n",
        "\n",
        "   ####### Parallel Mode ################################################\n",
        "   if npool > 1 and len(flist) > npool :\n",
        "        log('Parallel mode')\n",
        "        from utilmy.parallel  import multiproc_run\n",
        "        ll_list = multiproc_tochunk(flist, npool = npool)\n",
        "        multiproc_run(faiss_topk,  ll_list,  npool, verbose=True, start_delay= 5, \n",
        "                      input_fixed = { 'faiss_index': faiss_path }, )      \n",
        "        return 1\n",
        "   \n",
        "   ####### Single Mode #################################################\n",
        "   dirmap       = faiss_path.replace(\"faiss_trained\", \"map_idx\").replace(\".index\", '.parquet')  \n",
        "   map_idx_dict = db_load_dict(dirmap,  colkey = 'idx', colval = 'item_tag_vran' )\n",
        "\n",
        "   chunk  = 200000       \n",
        "   kk     = 0\n",
        "   os.makedirs(dir_out, exist_ok=True)    \n",
        "   dirout2 = dir_out \n",
        "   flist = [ t for t in flist if len(t)> 8 ]\n",
        "   log('\\n\\nN Files', len(flist), str(flist)[-100:]  ) \n",
        "   for fi in flist :\n",
        "       if os.path.isfile( dir_out + \"/\" + fi.split(\"/\")[-1] ) : continue\n",
        "       # nrows= 5000\n",
        "       df = pd_read_file( fi, n_pool=1  ) \n",
        "       df = df.iloc[:nrows, :]\n",
        "       log(fi, df.shape)\n",
        "       df = df.sort_values('id') \n",
        "\n",
        "       dfall  = pd.DataFrame()   ;    nchunk = int(len(df) // chunk)    \n",
        "       for i in range(0, nchunk+1):\n",
        "           if i*chunk >= len(df) : break         \n",
        "           i2 = i+1 if i < nchunk else 3*(i+1)\n",
        "        \n",
        "           x0 = np_str_to_array( df[colemb].iloc[ i*chunk:(i2*chunk)].values   , l2_norm=True ) \n",
        "           log('X topk') \n",
        "           topk_dist, topk_idx = faiss_index.search(x0, topk)            \n",
        "           log('X', topk_idx.shape) \n",
        "                \n",
        "           dfi                   = df.iloc[i*chunk:(i2*chunk), :][[ colid ]]\n",
        "           dfi[ f'{colid}_list'] = np_matrix_to_str2( topk_idx, map_idx_dict)  ### to item_tag_vran           \n",
        "           # dfi[ f'dist_list']  = np_matrix_to_str( topk_dist )\n",
        "           dfi[ f'sim_list']     = np_matrix_to_str_sim( topk_dist )\n",
        "        \n",
        "           dfall = pd.concat((dfall, dfi))\n",
        "\n",
        "       dirout2 = dir_out + \"/\" + fi.split(\"/\")[-1]      \n",
        "       # log(dfall['id_list'])\n",
        "       pd_to_file(dfall, dirout2, show=1)  \n",
        "       kk    = kk + 1\n",
        "       if kk == 1 : dfall.iloc[:100,:].to_csv( dirout2.replace(\".parquet\", \".csv\")  , sep=\"\\t\" )\n",
        "             \n",
        "   log('All finished')    \n",
        "   return os.path.dirname( dirout2 )\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################################################\n",
        "if 'utils_matplotlib':\n",
        "    CSS = \"\"\"\n",
        "        text.mpld3-text, div.mpld3-tooltip {\n",
        "          font-family:Arial, Helvetica, sans-serif;\n",
        "        }\n",
        "        g.mpld3-xaxis, g.mpld3-yaxis {\n",
        "        display: none; }\n",
        "        \"\"\"\n",
        "\n",
        "    class TopToolbar(mpld3.plugins.PluginBase):\n",
        "        \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
        "\n",
        "        JAVASCRIPT = \"\"\"\n",
        "        mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
        "        TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
        "        TopToolbar.prototype.constructor = TopToolbar;\n",
        "        function TopToolbar(fig, props){\n",
        "            mpld3.Plugin.call(this, fig, props);\n",
        "        };\n",
        "        TopToolbar.prototype.draw = function(){\n",
        "          // the toolbar svg doesn't exist\n",
        "          // yet, so first draw it\n",
        "          this.fig.toolbar.draw();\n",
        "          // then change the y position to be\n",
        "          // at the top of the figure\n",
        "          this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
        "          this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
        "          // then remove the draw function,\n",
        "          // so that it is not called again\n",
        "          this.fig.toolbar.draw = function() {}\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self):\n",
        "            self.dict_ = {\"type\": \"toptoolbar\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if 'utils_vector':\n",
        "    def np_array_to_str(vv, ):\n",
        "        \"\"\" array/list into  \",\" delimited string \"\"\"\n",
        "        vv= np.array(vv, dtype='float32')\n",
        "        vv= [ str(x) for x in vv]\n",
        "        return \",\".join(vv)\n",
        "\n",
        "\n",
        "    def np_str_to_array(vv,  l2_norm=True,     mdim = 200):\n",
        "        \"\"\" Convert list of string into numpy 2D Array\n",
        "        Docs::\n",
        "             \n",
        "             np_str_to_array(vv=[ '3,4,5', '7,8,9'],  l2_norm=True,     mdim = 3)    \n",
        "\n",
        "        \"\"\"\n",
        "        from sklearn import preprocessing\n",
        "        import faiss\n",
        "        X = np.zeros(( len(vv) , mdim  ), dtype='float32')\n",
        "        for i, r in enumerate(vv) :\n",
        "            try :\n",
        "              vi      = [ float(v) for v in r.split(',')]\n",
        "              X[i, :] = vi\n",
        "            except Exception as e:\n",
        "              log(i, e)\n",
        "\n",
        "        if l2_norm:\n",
        "            # preprocessing.normalize(X, norm='l2', copy=False)\n",
        "            faiss.normalize_L2(X)  ### Inplace L2 normalization\n",
        "            log(\"Normalized X\")\n",
        "        return X\n",
        "\n",
        "\n",
        "    def np_matrix_to_str2(m, map_dict:dict):\n",
        "        \"\"\" 2D numpy into list of string and apply map_dict.\n",
        "        \n",
        "        Doc::\n",
        "            map_dict = { 4:'four', 3: 'three' }\n",
        "            m= [[ 0,3,4  ], [2,4,5]]\n",
        "            np_matrix_to_str2(m, map_dict)\n",
        "\n",
        "        \"\"\"\n",
        "        res = []\n",
        "        for v in m:\n",
        "            ss = \"\"\n",
        "            for xi in v:\n",
        "                ss += str(map_dict.get(xi, \"\")) + \",\"\n",
        "            res.append(ss[:-1])\n",
        "        return res    \n",
        "\n",
        "\n",
        "    def np_matrix_to_str(m):\n",
        "        res = []\n",
        "        for v in m:\n",
        "            ss = \"\"\n",
        "            for xi in v:\n",
        "                ss += str(xi) + \",\"\n",
        "            res.append(ss[:-1])\n",
        "        return res            \n",
        "                \n",
        "    \n",
        "    def np_matrix_to_str_sim(m):   ### Simcore = 1 - 0.5 * dist**2\n",
        "        res = []\n",
        "        for v in m:\n",
        "            ss = \"\"\n",
        "            for di in v:\n",
        "                ss += str(1-0.5*di) + \",\"\n",
        "            res.append(ss[:-1])\n",
        "        return res   \n",
        "\n",
        "\n",
        "    def os_unzip(dirin, dirout):\n",
        "        # !/usr/bin/env python3\n",
        "        import sys\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(dirin, 'r') as zip_ref:\n",
        "            zip_ref.extractall(dirout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if 'custom_code':\n",
        "    def pd_add_onehot_encoding(dfref, img_dir, labels_col):\n",
        "        \"\"\"\n",
        "           id, uri, cat1, cat2, .... , cat1_onehot\n",
        "\n",
        "        \"\"\"\n",
        "        import glob\n",
        "        fpaths = glob.glob(img_dir)\n",
        "        fpaths = [fi for fi in fpaths if \".\" in fi.split(\"/\")[-1]]\n",
        "        log(str(fpaths)[:100])\n",
        "\n",
        "        df = pd.DataFrame(fpaths, columns=['uri'])\n",
        "        log(df.head(1).T)\n",
        "        df['id'] = df['uri'].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
        "        df['id'] = df['id'].apply(lambda x: int(x))\n",
        "        df = df.merge(dfref, on='id', how='left')\n",
        "\n",
        "        # labels_col = [  'gender', 'masterCategory', 'subCategory', 'articleType' ]\n",
        "\n",
        "        for ci in labels_col:\n",
        "            dfi_1hot = pd.get_dummies(df, columns=[ci])  ### OneHot\n",
        "            dfi_1hot = dfi_1hot[[t for t in dfi_1hot.columns if ci in t]]  ## keep only OneHot\n",
        "            df[ci + \"_onehot\"] = dfi_1hot.apply(lambda x: ','.join([str(t) for t in x]), axis=1)\n",
        "            #####  0,0,1,0 format   log(dfi_1hot)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "    def topk_custom(topk=100, dirin=None, pattern=\"df_*\", filter1=None):\n",
        "        \"\"\"  python prepro.py  topk    |& tee -a  /data/worpoch_261/topk/zzlog.py\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        from utilmy import pd_read_file\n",
        "        import cv2\n",
        "\n",
        "        filter1 = \"all\"    #### \"article\"\n",
        "\n",
        "        dirout  = dirin + \"/topk/\"\n",
        "        os.makedirs(dirout, exist_ok=True)\n",
        "        log(dirin)\n",
        "\n",
        "        #### Load emb data  ###############################################\n",
        "        df        = pd_read_file(  dirin + f\"/{pattern}.parquet\", n_pool=10 )\n",
        "        log(df)\n",
        "        df['id1'] = df['id'].apply(lambda x : x.split(\".\")[0])\n",
        "\n",
        "\n",
        "        #### Element X0 ######################################################\n",
        "        colsx = [  'masterCategory', 'subCategory', 'articleType' ]  # 'gender', , 'baseColour' ]\n",
        "        df0   = df.drop_duplicates( colsx )\n",
        "        log('Reference images', df0)\n",
        "        llids = list(df0.sample(frac=1.0)['id'].values)\n",
        "\n",
        "\n",
        "        for idr1 in llids :\n",
        "            log(idr1)\n",
        "            #### Elements  ####################################################\n",
        "            ll = [  (  idr1,  'all'     ),\n",
        "                    # (  idr1,  'article' ),\n",
        "                    (  idr1,  'color'   )\n",
        "            ]\n",
        "\n",
        "\n",
        "            for (idr, filter1) in ll :\n",
        "                dfi     = df[ df['id'] == idr ]\n",
        "                log(dfi)\n",
        "                if len(dfi) < 1: continue\n",
        "                x0      = np.array(dfi['pred_emb'].values[0])\n",
        "                xname   = dfi['id'].values[0]\n",
        "                log(xname)\n",
        "\n",
        "                #### 'gender',  'masterCategory', 'subCategory',  'articleType',  'baseColour',\n",
        "                g1 = dfi['gender'].values[0]\n",
        "                g2 = dfi['masterCategory'].values[0]\n",
        "                g3 = dfi['subCategory'].values[0]\n",
        "                g4 = dfi['articleType'].values[0]\n",
        "                g5 = dfi['baseColour'].values[0]\n",
        "                log(g1, g2, g3, g4, g5)\n",
        "\n",
        "                xname = f\"{g1}_{g4}_{g5}_{xname}\".replace(\"/\", \"-\")\n",
        "\n",
        "                if filter1 == 'article' :\n",
        "                    df1 = df[ (df.articleType == g4) ]\n",
        "\n",
        "                if filter1 == 'color' :\n",
        "                    df1 = df[ (df.gender == g1) & (df.subCategory == g3) & (df.articleType == g4) & (df.baseColour == g5)  ]\n",
        "                else :\n",
        "                    df1 = copy.deepcopy(df)\n",
        "                    #log(df)\n",
        "\n",
        "                ##### Setup Faiss queey ########################################\n",
        "                x0      = x0.reshape(1, -1).astype('float32')\n",
        "                vectors = np.array( list(df1['pred_emb'].values) )\n",
        "                log(x0.shape, vectors.shape)\n",
        "\n",
        "                dist, rank = topk_nearest_vector(x0, vectors, topk= topk)\n",
        "                # print(dist)\n",
        "                df1              = df1.iloc[rank[0], :]\n",
        "                df1['topk_dist'] = dist[0]\n",
        "                df1['topk_rank'] = np.arange(0, len(df1))\n",
        "                log( df1 )\n",
        "                df1.to_csv( dirout + f\"/topk_{xname}_{filter1}.csv\"  )\n",
        "\n",
        "                img_list = df1['id'].values\n",
        "                log(str(img_list)[:30])\n",
        "\n",
        "                log('### Writing images on disk  ###########################################')\n",
        "                import diskcache as dc\n",
        "                # db_path = \"/data/workspaces/noelkevin01/img/data/fashion/train_npz/small/img_train_r2p2_70k_clean_nobg_256_256-100000.cache\"\n",
        "                db_path = \"/dev/shm/train_npz/small//img_train_r2p2_1000k_clean_nobg_256_256-1000000.cache\"\n",
        "                cache   = dc.Cache(db_path)\n",
        "                print('Nimages', len(cache) )\n",
        "\n",
        "                dir_check = dirout + f\"/{xname}_{filter1}/\"\n",
        "                os.makedirs(dir_check, exist_ok=True)\n",
        "                for i, key in enumerate(img_list) :\n",
        "                    if i > 15: break\n",
        "                    img  = cache[key]\n",
        "                    img  = img[:, :, ::-1]\n",
        "                    key2 = key.split(\"/\")[-1]\n",
        "                    cv2.imwrite( dir_check + f\"/{i}_{key2}\"  , img)\n",
        "                log( dir_check )\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        " \n",
        "    \n",
        "###############################################################################################################\n",
        "if __name__ == \"__main__\":\n",
        "    import fire\n",
        "    fire.Fire()\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "btU6LeufQHnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "65fc0a33-4ea7-43ec-faba-ae5f4aa94540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e0996cd7edf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmpld3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhierarchy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdendrogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpld3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2-1o30BgVUBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S_fhYihlVUFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) New code\n"
      ],
      "metadata": {
        "id": "5BmKDE1QVVLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test1() -> None:\n",
        "    \"\"\"function test1     \n",
        "    \"\"\"\n",
        "    d = Box({})\n",
        "    dirtmp =\"./ztmp/\"\n",
        "    embedding_create_vizhtml(dirin=dirtmp + \"/model.vec\", dirout=dirtmp + \"out/\", dim_reduction='umap', nmax=100, ntrain=10)\n",
        "\n",
        "\n",
        "\n",
        "def test2():\n",
        "    ### new tests\n",
        "    pass\n",
        "\n",
        "\n",
        "def test3():\n",
        "    ### new tests\n",
        "    pass\n",
        "\n",
        "\n",
        "def test4():\n",
        "    ### new tests\n",
        "    pass\n",
        "\n",
        "\n",
        "def test5():\n",
        "    ### new tests\n",
        "    pass"
      ],
      "metadata": {
        "id": "Cql8TfusVUIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "leOstN5_igrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iQSifGJMZKLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X77gZMJIWRz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qohywANIPfOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xQdlWvRqClxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zqv3Sf6-VesL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vHphz5loVeuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Lc5gcyTDVexm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6H4dEvONVe0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iSW3dOAhVvDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cHCqzzj6WaHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aRTnuwjjWaKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "woZiHlbQVvGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NJR_zmpuVvJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wUcjHJkrVvL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dtx6mdQGVvOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ErqBg_RXVvRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NXPmVRa8Ve3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Lkr5x0v8Ve6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}